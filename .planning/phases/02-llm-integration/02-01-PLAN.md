---
phase: 02-llm-integration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/pom.xml
  - backend/src/main/resources/application.yml
  - backend/src/main/resources/application-dev.yml
  - backend/src/main/java/com/vocab/bulgarian/llm/config/AsyncConfig.java
  - backend/src/main/java/com/vocab/bulgarian/llm/config/CacheConfig.java
  - backend/src/main/java/com/vocab/bulgarian/llm/config/LlmConfig.java
  - backend/src/main/java/com/vocab/bulgarian/llm/dto/LemmaDetectionResponse.java
  - backend/src/main/java/com/vocab/bulgarian/llm/dto/InflectionSet.java
  - backend/src/main/java/com/vocab/bulgarian/llm/dto/LemmaMetadata.java
autonomous: true

must_haves:
  truths:
    - "Spring AI Ollama starter is on the classpath and auto-configures ChatClient.Builder"
    - "Redis cache (Valkey-compatible) is configured with named caches for LLM response types"
    - "ThreadPoolTaskExecutor is configured for async LLM execution"
    - "Resilience4j circuit breaker is configured for Ollama instance"
    - "DTO records exist for all three LLM response types (lemma detection, inflections, metadata)"
  artifacts:
    - path: "backend/pom.xml"
      provides: "Spring AI Ollama, Spring Data Redis, Resilience4j, AOP dependencies"
      contains: "spring-ai-starter-model-ollama"
    - path: "backend/src/main/java/com/vocab/bulgarian/llm/config/AsyncConfig.java"
      provides: "ThreadPoolTaskExecutor for async LLM calls"
      contains: "llmTaskExecutor"
    - path: "backend/src/main/java/com/vocab/bulgarian/llm/config/CacheConfig.java"
      provides: "RedisCacheManager with named caches"
      contains: "lemmaDetection"
    - path: "backend/src/main/java/com/vocab/bulgarian/llm/config/LlmConfig.java"
      provides: "ChatClient bean with BgGPT model defaults"
      contains: "ChatClient"
    - path: "backend/src/main/java/com/vocab/bulgarian/llm/dto/LemmaDetectionResponse.java"
      provides: "Structured output for lemma detection"
      contains: "record LemmaDetectionResponse"
    - path: "backend/src/main/java/com/vocab/bulgarian/llm/dto/InflectionSet.java"
      provides: "Structured output for inflection generation"
      contains: "record InflectionSet"
    - path: "backend/src/main/java/com/vocab/bulgarian/llm/dto/LemmaMetadata.java"
      provides: "Structured output for metadata generation"
      contains: "record LemmaMetadata"
  key_links:
    - from: "backend/src/main/resources/application.yml"
      to: "Resilience4j circuit breaker"
      via: "resilience4j.circuitbreaker.instances.ollama YAML config"
      pattern: "resilience4j.*circuitbreaker.*ollama"
    - from: "backend/src/main/resources/application-dev.yml"
      to: "Spring AI Ollama"
      via: "spring.ai.ollama properties"
      pattern: "spring\\.ai\\.ollama"
    - from: "backend/src/main/java/com/vocab/bulgarian/llm/config/LlmConfig.java"
      to: "Spring AI ChatClient"
      via: "ChatClient.Builder injection and bean creation"
      pattern: "ChatClient\\.Builder"
---

<objective>
Add Spring AI Ollama dependencies, configure async execution, Redis/Valkey caching, Resilience4j circuit breaker, and create structured output DTOs for all three LLM operations.

Purpose: Establish the configuration foundation that all LLM service classes depend on -- dependencies, beans, caching, resilience, and response schemas.
Output: Configured Spring Boot application with Spring AI, async, Redis cache, circuit breaker ready; DTO records for structured LLM output.
</objective>

<execution_context>
@/Users/kevin/.claude/get-shit-done/workflows/execute-plan.md
@/Users/kevin/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-llm-integration/02-RESEARCH.md
@.planning/phases/01-foundation-and-data-model/01-01-SUMMARY.md
@.planning/phases/01-foundation-and-data-model/01-02-SUMMARY.md

# Existing files to modify
@backend/pom.xml
@backend/src/main/resources/application.yml
@backend/src/main/resources/application-dev.yml

# Existing domain model (DTOs must align with these enums)
@backend/src/main/java/com/vocab/bulgarian/domain/enums/PartOfSpeech.java
@backend/src/main/java/com/vocab/bulgarian/domain/enums/DifficultyLevel.java
@backend/src/main/java/com/vocab/bulgarian/domain/enums/ReviewStatus.java
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add Spring AI, caching, and resilience dependencies to pom.xml</name>
  <files>backend/pom.xml</files>
  <action>
Add the following dependencies to pom.xml:

1. **Spring AI BOM** in `<dependencyManagement>`:
   - `org.springframework.ai:spring-ai-bom:1.1.0-M3` (type: pom, scope: import)

2. **Spring AI Ollama starter** in `<dependencies>`:
   - `org.springframework.ai:spring-ai-starter-model-ollama` (no version -- managed by BOM)

3. **Spring Boot Cache starter**:
   - `org.springframework.boot:spring-boot-starter-cache`

4. **Spring Data Redis** (Valkey-compatible):
   - `org.springframework.boot:spring-boot-starter-data-redis`

5. **Resilience4j Spring Boot 3 starter**:
   - `io.github.resilience4j:resilience4j-spring-boot3`

6. **Spring Boot AOP** (required for Resilience4j annotations):
   - `org.springframework.boot:spring-boot-starter-aop`

7. **Spring AI BOM repository** -- add `<repositories>` section for Spring AI milestones:
   ```xml
   <repositories>
       <repository>
           <id>spring-milestones</id>
           <name>Spring Milestones</name>
           <url>https://repo.spring.io/milestone</url>
           <snapshots>
               <enabled>false</enabled>
           </snapshots>
       </repository>
   </repositories>
   ```

Do NOT change Java version or any existing dependencies. Keep existing structure intact.
  </action>
  <verify>Run `cd /Users/kevin/projects/bulgarian-vocabulary/backend && ./mvnw dependency:resolve -q` to confirm all dependencies resolve successfully. If mvnw doesn't exist, use `mvn dependency:resolve -q`.</verify>
  <done>All 6 new dependencies resolve; pom.xml compiles without errors; Spring AI BOM manages the Ollama starter version.</done>
</task>

<task type="auto">
  <name>Task 2: Configure Spring AI, async, cache, circuit breaker, and create DTO records</name>
  <files>
    backend/src/main/resources/application.yml
    backend/src/main/resources/application-dev.yml
    backend/src/main/java/com/vocab/bulgarian/llm/config/AsyncConfig.java
    backend/src/main/java/com/vocab/bulgarian/llm/config/CacheConfig.java
    backend/src/main/java/com/vocab/bulgarian/llm/config/LlmConfig.java
    backend/src/main/java/com/vocab/bulgarian/llm/dto/LemmaDetectionResponse.java
    backend/src/main/java/com/vocab/bulgarian/llm/dto/InflectionSet.java
    backend/src/main/java/com/vocab/bulgarian/llm/dto/LemmaMetadata.java
  </files>
  <action>
**A) Update application.yml** -- Add Redis/Valkey, Resilience4j circuit breaker, and Spring AI defaults:

```yaml
# Add under existing spring: section
spring:
  ai:
    ollama:
      base-url: ${OLLAMA_BASE_URL:http://localhost:11434}
      chat:
        options:
          model: todorov/bggpt:9b
          temperature: 0.3
          num-ctx: 2048

  data:
    redis:
      host: ${REDIS_HOST:localhost}
      port: ${REDIS_PORT:6379}
      timeout: 2000ms

  cache:
    type: redis
    redis:
      time-to-live: 86400000  # 24 hours in milliseconds

# Add at root level
resilience4j:
  circuitbreaker:
    instances:
      ollama:
        sliding-window-size: 10
        sliding-window-type: COUNT_BASED
        failure-rate-threshold: 50
        wait-duration-in-open-state: 60s
        permitted-number-of-calls-in-half-open-state: 3
        minimum-number-of-calls: 5
        register-health-indicator: true
```

**B) Update application-dev.yml** -- Override with Mac Studio URL and dev-friendly settings. The existing `ai.ollama.base-url` line already exists; update it to use the Spring AI property path `spring.ai.ollama.base-url`. Add chat options override for dev:

```yaml
spring:
  ai:
    ollama:
      base-url: ${OLLAMA_BASE_URL:http://mac-studio.local:11434}
      chat:
        options:
          model: todorov/bggpt:9b
          temperature: 0.3

# Dev-friendly circuit breaker: shorter wait, smaller window
resilience4j:
  circuitbreaker:
    instances:
      ollama:
        wait-duration-in-open-state: 10s
        sliding-window-size: 5
        minimum-number-of-calls: 3
```

**C) Create AsyncConfig.java** in `com.vocab.bulgarian.llm.config`:
- `@Configuration` and `@EnableAsync`
- Bean `llmTaskExecutor` returning `ThreadPoolTaskExecutor`
  - corePoolSize: 4, maxPoolSize: 8, queueCapacity: 25
  - threadNamePrefix: "llm-async-"
  - rejectedExecutionHandler: CallerRunsPolicy
  - waitForTasksToCompleteOnShutdown: true, awaitTerminationSeconds: 60
- Implement `AsyncConfigurer` to set `getAsyncExecutor()` returning the executor
- Override `getAsyncUncaughtExceptionHandler()` with SLF4J error logging

**D) Create CacheConfig.java** in `com.vocab.bulgarian.llm.config`:
- `@Configuration` and `@EnableCaching`
- Bean `cacheManager(RedisConnectionFactory connectionFactory)` returning `RedisCacheManager` with three named caches:
  - "lemmaDetection", "inflectionGeneration", "metadataGeneration"
- RedisCacheConfiguration: `entryTtl(Duration.ofHours(24))`
- Key serialization: `StringRedisSerializer`
- Value serialization: `GenericJackson2JsonRedisSerializer`
- Build RedisCacheManager with default config and initial cache configurations

**E) Create LlmConfig.java** in `com.vocab.bulgarian.llm.config`:
- `@Configuration`
- Bean `chatClient(ChatClient.Builder builder)` returning `ChatClient`
  - Build from the auto-configured `ChatClient.Builder` (injected by Spring AI)
  - Apply default system message: "You are a Bulgarian language expert. Respond ONLY in valid JSON matching the requested format. Do not include explanations outside the JSON structure."

**F) Create DTO records:**

**LemmaDetectionResponse.java** in `com.vocab.bulgarian.llm.dto`:
```java
public record LemmaDetectionResponse(
    @NotBlank String wordForm,       // The original word form submitted
    @NotBlank String lemma,          // Detected lemma (dictionary form)
    String partOfSpeech,             // LLM's detected part of speech
    boolean detectionFailed          // True if detection failed (for fallback)
) {
    // Static factory for fallback
    public static LemmaDetectionResponse failed(String wordForm) {
        return new LemmaDetectionResponse(wordForm, null, null, true);
    }
}
```

**InflectionSet.java** in `com.vocab.bulgarian.llm.dto`:
```java
public record InflectionSet(
    @NotBlank String lemma,
    @NotBlank String partOfSpeech,
    @NotEmpty List<InflectionEntry> inflections
) {
    public record InflectionEntry(
        @NotBlank String text,          // The inflected form (Cyrillic)
        String grammaticalTags          // e.g., "1sg.pres.perf", "3pl.past.imperf"
    ) {}
}
```

**LemmaMetadata.java** in `com.vocab.bulgarian.llm.dto`:
```java
public record LemmaMetadata(
    @NotBlank String lemma,
    @NotBlank String partOfSpeech,     // Matches PartOfSpeech enum values
    String category,                    // Topic category (e.g., "food", "travel")
    @NotBlank String difficultyLevel   // Matches DifficultyLevel enum values
) {}
```

All DTOs use `jakarta.validation.constraints` annotations for Bean Validation integration.
  </action>
  <verify>
Run `cd /Users/kevin/projects/bulgarian-vocabulary/backend && ./mvnw compile -q` to confirm all new classes compile successfully. Verify directory structure: `find src/main/java/com/vocab/bulgarian/llm -type f -name "*.java" | sort` should show 6 files (3 config + 3 dto).
  </verify>
  <done>
Application compiles with Spring AI auto-configuration, async executor configured with pool limits, Caffeine cache with 3 named caches and 24h TTL, Resilience4j circuit breaker configured for Ollama, ChatClient bean with Bulgarian language system prompt, and 3 DTO records ready for BeanOutputConverter structured output.
  </done>
</task>

</tasks>

<verification>
1. `cd backend && ./mvnw dependency:resolve -q` resolves all dependencies including Spring AI and Spring Data Redis
2. `cd backend && ./mvnw compile -q` compiles without errors
3. `find backend/src/main/java/com/vocab/bulgarian/llm -type f -name "*.java" | sort` shows 6 Java files
4. `grep -r "spring-ai-starter-model-ollama" backend/pom.xml` confirms Spring AI dependency
5. `grep -r "spring-boot-starter-data-redis" backend/pom.xml` confirms Redis dependency
6. `grep -r "resilience4j" backend/src/main/resources/application.yml` confirms circuit breaker config
7. `grep -r "spring.data.redis" backend/src/main/resources/application.yml` confirms Redis config
8. `grep -r "@EnableAsync" backend/src/main/java/` confirms async configuration
9. `grep -r "@EnableCaching" backend/src/main/java/` confirms cache configuration
</verification>

<success_criteria>
- Spring AI Ollama starter on classpath with auto-configuration
- Spring Data Redis (Valkey-compatible) configured with localhost:6379 connection
- RedisCacheManager with 3 named caches (lemmaDetection, inflectionGeneration, metadataGeneration)
- Cache TTL configured to 24 hours with JSON serialization
- ThreadPoolTaskExecutor "llmTaskExecutor" with core=4, max=8, queue=25
- Resilience4j circuit breaker "ollama" with 50% failure threshold, 60s open state
- ChatClient bean built from auto-configured builder
- 3 DTO records with Jakarta validation annotations
- Application compiles without errors
</success_criteria>

<output>
After completion, create `.planning/phases/02-llm-integration/02-01-SUMMARY.md`
</output>
